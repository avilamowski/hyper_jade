# MLflow Logging en Hyper Jade

Este documento explica cÃ³mo usar el sistema de logging de MLflow implementado en el proyecto Hyper Jade para el seguimiento de mÃ©tricas y artefactos durante la evaluaciÃ³n de asignaciones.

## ğŸš€ CaracterÃ­sticas

- **Logging automÃ¡tico de mÃ©tricas** en cada agente del pipeline
- **Artefactos generados** (requerimientos, prompts, anÃ¡lisis)
- **MÃ©tricas de rendimiento** (tiempo de ejecuciÃ³n, tasas de Ã©xito)
- **ConfiguraciÃ³n flexible** para diferentes entornos
- **VisualizaciÃ³n de mÃ©tricas** con herramientas integradas

## ğŸ“Š MÃ©tricas Registradas

### Pipeline Principal
- `total_pipeline_time_seconds`: Tiempo total del pipeline
- `requirements_generated`: NÃºmero de requerimientos generados
- `prompts_generated`: NÃºmero de prompts generados
- `analyses_completed`: NÃºmero de anÃ¡lisis completados
- `pipeline_success_rate`: Tasa de Ã©xito del pipeline

### Agente de GeneraciÃ³n de Requerimientos
- `llm_generation_time_seconds`: Tiempo de generaciÃ³n del LLM
- `requirements_generated`: NÃºmero de requerimientos
- `total_generation_time_seconds`: Tiempo total de generaciÃ³n
- `requirements_per_second`: Requerimientos por segundo
- `requirement_X_length_chars`: Longitud de cada requerimiento
- `requirement_X_length_words`: NÃºmero de palabras por requerimiento

### Agente de GeneraciÃ³n de Prompts
- `llm_generation_time_seconds`: Tiempo de generaciÃ³n del LLM
- `template_length_chars`: Longitud del template
- `template_length_lines`: NÃºmero de lÃ­neas del template
- `total_generation_time_seconds`: Tiempo total de generaciÃ³n
- `template_generation_rate`: Tasa de generaciÃ³n de templates
- `prompt_X_template_length_chars`: Longitud de cada prompt
- `prompt_X_has_code_variable`: Si el prompt tiene la variable `{{ code }}`

### Agente de CorrecciÃ³n de CÃ³digo
- `template_render_time_seconds`: Tiempo de renderizado del template
- `rendered_prompt_length_chars`: Longitud del prompt renderizado
- `llm_analysis_time_seconds`: Tiempo de anÃ¡lisis del LLM
- `analysis_length_chars`: Longitud del anÃ¡lisis
- `total_analysis_time_seconds`: Tiempo total de anÃ¡lisis
- `analysis_rate`: Tasa de anÃ¡lisis
- `requirement_X_result`: Resultado del anÃ¡lisis (1.0 = YES, 0.0 = NO)

## ğŸ› ï¸ ConfiguraciÃ³n

### Archivo de ConfiguraciÃ³n

El archivo `src/config/mlflow_config.yaml` permite configurar:

```yaml
mlflow:
  # Servidor de tracking
  tracking_uri: "file:./mlruns"  # Local (por defecto)
  # tracking_uri: "http://localhost:5000"  # Remoto
  
  # Nombre del experimento
  experiment_name: "assignment_evaluation"
  
  # ConfiguraciÃ³n de logging
  logging:
    level: "INFO"
    enable_console_logging: true
    enable_file_logging: false
    log_file_path: "./logs/mlflow.log"
  
  # ConfiguraciÃ³n de runs
  run:
    default_tags:
      project: "hyper_jade"
      version: "1.0.0"
```

### ConfiguraciÃ³n Local (Por Defecto)

```bash
# Los datos se guardan en ./mlruns/
mlflow ui --port 5000
```

### ConfiguraciÃ³n Remota

```yaml
mlflow:
  tracking_uri: "http://your-mlflow-server:5000"
  experiment_name: "assignment_evaluation"
```

## ğŸ“ˆ Uso

### 1. Ejecutar el Pipeline con Logging

```bash
# Ejecutar el pipeline normal (el logging es automÃ¡tico)
python main.py --assignment ejemplos/consigna.txt --code ejemplos/alu1.py

# Con contexto adicional
python main.py --assignment ejemplos/consigna.txt --code ejemplos/alu1.py --context "AnÃ¡lisis adicional"
```

### 2. Ver MÃ©tricas con MLflow UI

```bash
# Iniciar el servidor de MLflow UI
mlflow ui --port 5000

# Abrir en el navegador: http://localhost:5000
```

### 3. Usar el Script de VisualizaciÃ³n

```bash
# Listar todos los runs
python view_mlflow_metrics.py --list-runs

# Ver detalles de un run especÃ­fico
python view_mlflow_metrics.py --run-id <run_id>

# Comparar mÃºltiples runs
python view_mlflow_metrics.py --compare <run_id1> <run_id2>

# Graficar una mÃ©trica especÃ­fica
python view_mlflow_metrics.py --plot-metric "metrics.total_pipeline_time_seconds"

# Exportar datos de un run
python view_mlflow_metrics.py --export <run_id>
```

## ğŸ“ Estructura de Artefactos

Cada run de MLflow incluye los siguientes artefactos:

```
artifacts/
â”œâ”€â”€ assignment_description.txt          # DescripciÃ³n de la asignaciÃ³n
â”œâ”€â”€ requirements/                       # Requerimientos generados
â”‚   â”œâ”€â”€ requirement_01.txt
â”‚   â”œâ”€â”€ requirement_02.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ generated_templates/                # Templates de prompts
â”‚   â”œâ”€â”€ requirement_01.jinja
â”‚   â”œâ”€â”€ requirement_02.jinja
â”‚   â””â”€â”€ ...
â”œâ”€â”€ input_prompt_template.jinja        # Template de entrada
â”œâ”€â”€ input_student_code.py              # CÃ³digo del estudiante
â”œâ”€â”€ rendered_prompt.txt                # Prompt renderizado
â”œâ”€â”€ generated_analysis.txt             # AnÃ¡lisis generado
â””â”€â”€ output_analysis/                   # AnÃ¡lisis guardados
    â”œâ”€â”€ analysis_requirement_01.txt
    â”œâ”€â”€ analysis_requirement_02.txt
    â””â”€â”€ ...
```

## ğŸ” AnÃ¡lisis de MÃ©tricas

### MÃ©tricas de Rendimiento

- **Tiempo de Pipeline**: Monitorear el tiempo total de ejecuciÃ³n
- **Tiempo por Agente**: Identificar cuellos de botella
- **Tasa de Ã‰xito**: Verificar la confiabilidad del sistema

### MÃ©tricas de Calidad

- **Longitud de Requerimientos**: Evaluar la complejidad
- **Longitud de Prompts**: Verificar la claridad de instrucciones
- **Resultados de AnÃ¡lisis**: Seguimiento de resultados YES/NO

### MÃ©tricas de LLM

- **Tiempo de Respuesta**: Rendimiento del modelo
- **TamaÃ±o de Respuesta**: Complejidad de las respuestas
- **Tasa de GeneraciÃ³n**: Eficiencia del proceso

## ğŸš¨ Troubleshooting

### Problemas Comunes

1. **MLflow no puede conectarse al servidor**
   ```bash
   # Verificar que el servidor estÃ© corriendo
   mlflow ui --port 5000
   ```

2. **No se ven mÃ©tricas en la UI**
   - Verificar que el experimento estÃ© configurado correctamente
   - Revisar los logs de la aplicaciÃ³n

3. **Errores de permisos**
   ```bash
   # Crear directorio de logs si no existe
   mkdir -p logs
   chmod 755 logs
   ```

### Logs de Debug

```bash
# Habilitar logging detallado
export MLFLOW_LOG_LEVEL=DEBUG

# Ver logs de MLflow
tail -f logs/mlflow.log
```

## ğŸ“š Referencias

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [MLflow Tracking](https://mlflow.org/docs/latest/tracking.html)
- [MLflow UI](https://mlflow.org/docs/latest/tracking.html#tracking-ui)

## ğŸ¤ ContribuciÃ³n

Para agregar nuevas mÃ©tricas o artefactos:

1. Modificar el agente correspondiente en `src/agents/`
2. Agregar logging en el mÃ©todo principal
3. Actualizar este documento
4. Probar con el script de visualizaciÃ³n
